{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from mvodolagin_personal_imports import *\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ee70fd24a491b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from urllib.parse import urljoin\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"carolhouse_spider\"\n",
    "    start_urls = [\"http://www.carolhouse.com\"]\n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        # Look for 'Contact Us' link and yield a request with a callback\n",
    "        # Note: You might need to adjust the selector based on the actual site structure\n",
    "        contact_us_link = response.xpath('//a[contains(text(), \"Contact Us\")]/@href').get()\n",
    "        if contact_us_link:\n",
    "            yield response.follow(contact_us_link, self.parse_contact, meta={\"is_contact_page\": True})\n",
    "\n",
    "        # Find item links - adjust the selector to match your target site's structure\n",
    "        item_links = response.xpath('//a[contains(@href, \"item\")]/@href').getall()\n",
    "        for link in item_links:\n",
    "            yield response.follow(link, self.parse_item)\n",
    "\n",
    "        # Follow other potential links to pages like 'Contact Us' or item pages.\n",
    "        # This is a simplistic approach; you may need to refine how you select links to follow.\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_contact(self, response):\n",
    "        # Process the 'Contact Us' page here\n",
    "        # For example, extract the email address or form action\n",
    "        # Remember to check response.meta['is_contact_page'] if needed\n",
    "        yield {\n",
    "            \"url\": response.url,\n",
    "            \"is_contact_page\": response.meta.get(\"is_contact_page\", False),\n",
    "            # Add any other item fields you want to extract from the contact page\n",
    "        }\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        # Extract item details such as description and images\n",
    "        # Adjust selectors to match the structure of the item pages you're targeting\n",
    "        yield {\n",
    "            \"url\": response.url,\n",
    "            \"description\": response.xpath('//div[contains(@class, \"description\")]/text()').get(),\n",
    "            \"images\": response.xpath(\"//img/@src\").getall(),\n",
    "            # You might need to join image URLs with the base URL\n",
    "            \"images\": [urljoin(response.url, img) for img in response.xpath(\"//img/@src\").getall()],\n",
    "            # Add any other fields you want to extract\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78be014b6f56c377",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "554e461576908ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ca3b08248f7c5842",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "# Initialize and run the spider\n",
    "# Note: You might need to adjust get_project_settings() based on your Scrapy project setup or provide custom settings\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c35a51badfe328d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from twisted.internet import asyncioreactor\n",
    "# asyncioreactor.install()\n",
    "\n",
    "from scrapy import Selector\n",
    "from scrapy.http import Request\n",
    "from twisted.internet import defer, reactor\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "\n",
    "# Initialize the crawler runner with your project settings\n",
    "runner = CrawlerRunner(get_project_settings())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cef580a5a836999e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@defer.inlineCallbacks\n",
    "def fetch_page(url):\n",
    "    # The callback function to process the downloaded response\n",
    "    def parse(response):\n",
    "        return response\n",
    "    \n",
    "    # Schedule a request and return the response when it's done\n",
    "    yield runner.crawl(MySpider, start_urls=[url], parse=parse)\n",
    "\n",
    "# Example usage to fetch and store the response for inspection\n",
    "response = await fetch_page('https://www.carolhouse.com')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a2748f2e0eb6aed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fetch_page('https://www.carolhouse.com')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5687f7fe3f682a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "spider = MySpider()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef7c292f17635ef7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "qq = spider.parse('https://www.carolhouse.com')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d7cabd5f22b226",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "qq.__next__()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "972a9357c779ace5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f04e18da7f8cc736",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import requests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc26bce4801254",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://www.carolhouse.com/living-room/cabinets/room-type.aspx\"\n",
    "\n",
    "response = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b7153fb85c42040",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import HTML"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7dc8d3bcd6c8ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "HTML(response.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba949611eb02f7c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43db05f22e41ed89",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
